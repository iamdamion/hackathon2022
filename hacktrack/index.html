<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"> <meta name="author" content="OHBM Open-science Special Interest Group"> <meta name="description" content="OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG."> <meta name="keywords" content="brainhack, neuroscience, AI"> <!-- <meta name="google-site-verification" content="" /> <link rel="canonical" href="https://ohbm.github.io/hackathon2022"> --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content="@OhbmOpen"> <meta name="twitter:title" content="HackTrack"> <meta name="twitter:description" content="OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG."> <meta name="twitter:image:src" content="https://ohbm.github.io/hackathon2022/hackathon2022"> <meta property="og:title" content="HackTrack" /> <meta property="og:site_name" content="OHBM BrainHack 2022" /> <meta property="og:type" content="website" /> <meta property="og:url" content="https://ohbm.github.io/hackathon2022" /> <meta property="og:image" content="https://ohbm.github.io/hackathon2022/hackathon2022" /> <meta property="og:image:width" content="700" /> <meta property="og:image:height" content="350" /> <meta property="og:description" content="OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG." /> <title>HackTrack &bull; OHBM BrainHack 2022</title> <link rel="shortcut icon" href="/hackathon2022/img/favicons/favicon.ico"> <link rel="apple-touch-icon" sizes="152x152" href="/hackathon2022/img/favicons/apple-icon-152x152.png"> <link rel="apple-touch-icon" sizes="144x144" href="/hackathon2022/img/favicons/apple-icon-144x144.png"> <link rel="apple-touch-icon" sizes="120x120" href="/hackathon2022/img/favicons/apple-icon-120x120.png"> <link rel="apple-touch-icon" sizes="114x114" href="/hackathon2022/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="76x76" href="/hackathon2022/img/favicons/apple-icon-76x76.png"> <link rel="apple-touch-icon" sizes="72x72" href="/hackathon2022/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="60x60" href="/hackathon2022/img/favicons/apple-icon-60x60.png"> <link rel="apple-touch-icon" sizes="57x57" href="/hackathon2022/img/favicons/apple-icon-57x57.png"> <link rel="icon" type="image/png" href="/hackathon2022/img/favicons/favicon-96x96.png"> <link rel="icon" type="image/png" href="/hackathon2022/img/favicons/favicon-32x32.png"> <link rel="icon" type="image/png" href="/hackathon2022/img/favicons/favicon-16x16.png"> <meta name="msapplication-TileColor" content="#2b5797"> <meta name="msapplication-TileImage" content="/hackathon2022/img/favicons/mstile-144x144.png"> <meta name="msapplication-config" content="/hackathon2022/img/favicons/browserconfig.xml"> <meta name="theme-color" content="#2b5797"> <link href="/hackathon2022/css/main.css" rel="stylesheet"> <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries --> <!-- WARNING: Respond.js doesn't work if you view the page via file:// --> <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]--> </head> <body> <div id="preloader" class="preloader"> <div class="loader-gplus"></div> </div> <div id="st-container" class="st-container disable-scrolling"> <div class="st-pusher"> <div class="st-content"> <!-- Begin Top Section --> <section id="top-section" class="top-section image-section enable-overlay" style="background-image: url('/hackathon2022/img/sections-background/25900655562_27549efb87_o.jpg');"> <div class="overlay gradient-overlay"></div> <header id="top-header" class="top-header"> <div class="overlay white-solid"></div> <svg id="menu-trigger" class="menu-trigger icon icon-menu visible-xs visible-sm visible-md" viewBox="0 0 32 32"> <use xlink:href="/hackathon2022/img/sprites/sprites.svg#icon-menu"></use> </svg> <a href="/hackathon2022/" id="logo-header" class="logo-header"> <div class="logo logo-light"></div> </a> <nav class="st-menu st-effect" id="menu"> <div class="logo-navbar logo logo-dark visible-xs visible-sm"></div> <ul> <li> <a class="" href=" /hackathon2022/schedule " >Schedule</a> </li> <li> <a class="" href=" /hackathon2022/hacktrack " >HackTrack</a> </li> <li> <a class="" href=" /hackathon2022/traintrack/ " >TrainTrack Corner</a> </li> <li> <a class="" href=" /hackathon2022/buddy-system/ " >Buddy System</a> </li> </ul> <ul id="bottom-navlinks" class="bottom-navlinks visible-xs visible-sm"> </ul> </nav> </header> <div class="content-wrapper"> <div class="jumbotron text-left"> <div class="animated hiding" data-animation="fadeInLeft" data-delay="500"> <h1>HackTrack</h1> </div> </div> </div> </section> <!-- End Top Section --> <!-- About Hackathon Section --> <section id="about-hackathon" class="about-hackathon"> <div class="content-wrapper"> <div class="col-md-8 col-md-offset-2"> <h3>HackTrack projects</h3> <div style="text-align: justify;"> <h6> <br>The <b>HackTrack</b> is the official <b>fun side</b> of a <b>Brainhack</b> event, where people can work together on projects. What projects? Any kind! From <a href="https://github.com/ohbm/hackathon2020/issues/124" target="_blank">exploding brains</a> to <a href="https://github.com/ohbm/hackathon2020/issues/166" target="_blank">resource gathering</a> and <a href="https://github.com/ohbm/hackathon2020/issues/156" target="_blank">data sharing</a>!<br><br> Would you like to propose a project? Just open <a href="https://github.com/ohbm/hackathon2022/issues/new/choose" target="_blank"> an issue on our GitHub repository</a> and fill the template, we will be in touch to help you get going! But be sure to <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageid=4073" target="_blank"> register</a> first!<br><br> Do you plan to focus on visualization? Are you getting images so weird that they are kind of beautiful? Consider participating in the <em>Beautiful Mistake</em> category of the <a href="https://ohbm-brainart.github.io" target="_blank">BrainArt SIG</a> competition! You can find the submission form <a href="https://docs.google.com/forms/d/e/1FAIpQLSdkfoq-VF_Aw27MD1whBAZCFl6BHldOpOAQ2GWSXFng7fD3Vw/viewform" target="_blank">here</a>.<br><br> </h6> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://neuroscout.org/static/neuroscout_simpler_dark_blue_medium.svg" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neuroscout/neuroscout-cli/">Neuroscout: A platform for fast and flexible re-analysis of (naturalistic) fMRI studies</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Neuroscout has two primary goals: 1) to democratize reproducible fMRI analysis by making it trivially easy to specify and fit models to public fMRI datasets and 2) facilitate the analysis of naturalistic datasets by leveraging machine-learning algorithms for automated annotation. Combined, these two goals seek to increase the reproducibility and generalizability of neuroimaging analysis. Neuroscout is already a stable platform. However, we are still working on growing our user base, and expanding the functionality. To that end we propose several goals for this hackathon: 1) Develop and get feedback on a end-to-end tutorial for Neuroscout. Currently, although Neuroscout features sufficient documentation, many users are not yet clear on how to best use the platform. By creating a complete end-to-end tutorial, and receiving feedback from the community, we aim to Neuroscout more accessible. This is a great issue for first time contributors! 2) Add more datasets. Neuroscout currently spans 40 distinct naturalistic stimuli across over a dozen independent datasets. However, many more datasets are made public yearly, and we will seek to further expand the number of datasets indexed. In particular, we hope to also include non-naturalistic datasets, to increase the scope of Neurocout. 3) Add more naturalistic features. Neuroscout uses machine-learning algorithms to annotate naturalistic stimuli such as movies. We developed a library (<em>pliers</em>) to provide a uniform API to diverse algorithms. However, there are many more algorithms that could be incorporated and tested. Many are available to extract but have not been actively worked on. A specific example would be to incorporate DeepGaze, to simulate eye tracking data in datasets without it. Validating new features by building models would be a relatively easy contribution for first timers. 4) Develop multivariate analysis pipelines. Neuroscout currently focuses on fitting multi-stage univariate GLM models using BIDS Stats Models (a formal specification). However, multivariate approaches are widely popular for analyzing naturalistic data. Although we don’t aim to fully specify multivariate models in a standardized format, we aim to prototype Neuroscout-compatible multivariate workflows that can take advantage of the vast number of datasets and features made easily available by the Neuroscout API. In the future, this project could be refined to become a core component of Neuroscout, as an alternative to GLM models.</p> </p> <h5>SKILLS</h5> <p><p>Basic familiarity with neuroimaging analysis. Plus: tutorial writing skills, experience with naturalistic data and multivariate modeling (specifically encoding and decoding models).</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://github.com/neurobagel/browbids/blob/main/public/browbids.png?raw=true" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurobagel/browbids">Make the browser run pybids</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Browsers (typically) don’t run Python code. That’s a shame, because the neuroimaging community has so many great tools written in Python that could be reused in interactive, graphical web apps without requiring users to install anything on their machine. If you’re working with BIDS, you are probably familiar with the <a href="https://bids-standard.github.io/bids-validator/">bids-validator</a> project that you can simply access with your web browser. Wouldn’t it be nice if you could just run some simple pyBIDS queries on you local BIDS dataset like that, directly in the browser, without having to first source your python environment and installing some libraries? And ideally without having to reimplement pyBIDS as a javascript library? Luckily, new solutions for python in the browser are currently being developed: <a href="https://pyodide.org/en/stable/">Pyodide</a> and <a href="https://pyscript.net/">pyscript</a> are two (fairly) recent efforts to bring python to the browser using the <a href="https://webassembly.org/">webassembly</a> language. These projects not only allow us to run Python code in the browser and use the output, but we can also install (some) Python libraries from PyPI and use them directly. So you can install <a href="https://github.com/ANCPLabOldenburg/ancp-bids">ancp_bids</a> inside pyodide, and then load a local BIDS dataset in your browser and run some basic queries. Here is a very simple prototype: https://browbids.netlify.app/ . But there are still a lot of challenges to solve, particularly with file system access and python dependencies that don’t play nice with pyodide. The rough <strong>goals for this hackathon</strong> are:</p> <ul> <li>understand what the most relevant pyBIDS use cases and queries are that can be implemented in the browser</li> <li>find out how we can make a reusable wrapper / plugin of the pyodide-pyBIDS bundle that other projects can just load to gain this functionality</li> <li>investigate ways to safely access the local filesystem so we can expose file metadata or even content to the python instance</li> <li>document what we have learned for other projects that may be also interested in browserizing a python library</li> <li>show a minimal but useful prototype of parsing a BIDS dataset in the browser using pyBIDS</li> </ul> </p> <h5>SKILLS</h5> <p><p>We are very much starting from the beginning (although there is a simple proof of concept) and are trying to find a good way to address this project. Many of the initial challenges will probably require experience with javascript and web development as well as pyBIDS, but there is also a need for a BIDS user perspective to answer understand what would be useful things to do with pyBIDS in the browser. So any combination of</p> <ul> <li>experience using BIDS (to suggest or discuss use cases)</li> <li>good at writing accessible documentation / tutorials</li> <li>experience with Javascript (JS), any of</li> <li>ideally some accessible frontend framework (e.g. Vue)</li> <li>visualization libraries (e.g. d3)</li> <li>JS testing (e.g. Jest, cypress.io)</li> <li>packaging or distribution (e.g. npm / Vue plugin writing, …)</li> <li>browser filesystems, accessing local filesystem from browser,</li> <li>python</li> <li>familiarity with pybids / acnp_bids python API</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://compose.neurosynth.org/static/synth.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurostuff/neurostore">Neurosynth-Compose User Testing</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://compose.neurosynth.org">neurosynth-compose</a> is a reimagining of the original neurosynth to give users the power to edit studies and create their own meta-analyses. While it is possible to create and execute a meta-analysis, we do not know what features/fixes people want. Goals:</p> <ul> <li>get people to test the platform and identify pain points</li> <li>identify places where documentation is unclear/missing</li> </ul> </p> <h5>SKILLS</h5> <p><ul> <li>interest in performing a meta-analysis</li> <li>patience to work with our website</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://nimare.readthedocs.io/en/latest/_images/nimare_overview.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurostuff/NiMARE">NiMARE: Neuroimaging Meta-Analytic Research Environment</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><h4 id="user-guide-project">User Guide Project</h4> <p>Currently, there is a mix of methods descriptions (the <a href="https://nimare.readthedocs.io/en/latest/methods.html">NiMARE Methods</a> page) and <a href="https://nimare.readthedocs.io/en/latest/auto_examples/index.html">examples</a> of NiMARE’s functionality, but those two elements are very separate, and the examples are more a smattering of disconnected exhibitions than a tutorial. We came up with the idea of making the documentation more into a user guide.</p> <h4 id="resources">Resources</h4> <ul class="task-list"> <li>https://nimare.readthedocs.io/en/latest/methods.html</li> <li>https://nimare.readthedocs.io/en/latest/auto_examples/index.html</li> <li>https://github.com/neurostuff/ohbm2021-nimare-tutorial</li> <li>https://github.com/NBCLab/nimare-paper <h4 id="user-guide-goals">User Guide Goals</h4> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Create a jupyter book</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Leverage existing resources (see above)</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Write tutorial documentation with a novice meta-analysis practicitioner in mind</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Have people test jupyter book for typos/understanding/correctness <h4 id="extension-addition-project">Extension addition project</h4> <p>NiMARE covers a wide breadth of meta-analytic algorithms. However, the only constant is change. With new algorithms emerging from research across the globe, there is a need for integrating the latest trends in neuroimaging meta-analysis and making the process smooth for new contributors. In order to kickstart the extension process the goals are twofold:</p> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />integrate a new algorithm into NiMARE</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />document the process and find pain points</li> </ul> </p> <h5>SKILLS</h5> <p><p>Minimally, one of the following:</p> <ul> <li>meta-analysis: beginner</li> <li>python: beginner</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/nipype/pydra/master/docs/logo/pydra_logo.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nipype/pydra">Pydra: Converting existing scientiﬁc workﬂows to the new dataﬂow engine Pydra</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Pydra is a part of the second generation of the Nipype ecosystem. The goal of this project is to convert existing neuroimaging analytical workﬂows that are written in other languages to the new dataﬂow engine Pydra. The main deliverables of the project are comprehensive Pydra workﬂows that use interfaces from neuroimaging packages such as FSL, SPM, ANTs, AFNI, FreeSurfer. Goals for the Brainhack:</p> <ul> <li>Converting <a href="https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_bids_features.html#sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py">Nilearn.glm</a> tutorial to Pydra</li> <li>Moving <a href="https://github.com/poldracklab/fitlins">FitLins</a> to Pydra</li> </ul> </p> <h5>SKILLS</h5> <p><p>Minimal:</p> <ul> <li>Python</li> <li>Git Optional:</li> <li>Familiarity with GLM, BIDS</li> <li>Familiarity with Nipype, Nilearn</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/niivue/niivue-ui">NiiVue: designing a user interface for mobile, tablet, and desktop</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://github.com/niivue/niivue">NiiVue</a> is brain imaging visualisation software that works on any device and all modern web browsers. NiiVue can read and display more than 30 volume and surface file types used in neuroimaging. NiiVue is designed to be a minimal <em>library</em> that other <strong>developers</strong> can use. However, we want to create a full-featured user interface with the bells and whistles that most <strong>users</strong> would expect from well designed visualisation software. The user interface will be friendly, inviting, interactive, and accessible (all things we expect from modern software). We will use the following frameworks, languages, and projects:</p> <ul> <li><a href="https://reactjs.org">React</a> functional components</li> <li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript">Javascript</a></li> <li><a href="https://mui.com">Material UI </a></li> <li><a href="https://vitejs.dev">Vite</a></li> <li><a href="https://github.com/niivue/niivue">NiiVue</a> We have put a lot of effort into the NiiVue library, but our <a href="https://github.com/niivue/niivue-ui">demo user interface</a> is not yet full-featured. This user interface <strong>MUST</strong> work on mobile, tablet, and desktop browsers and will support both touch interfaces and mouse/keyboard interfaces. At the BrainHack we will:</li> <li>collaboratively design the user interface in <a href="https://www.figma.com">Figma</a> <em>before</em> coding (starting from an initial design from @hanayik)</li> <li>outline the steps to create the UI as a list of Github issues</li> <li>implement the UI design and interactivity by working on our predefined list of issues</li> <li>test and re-design as needed</li> </ul> </p> <h5>SKILLS</h5> <p><p>There will be plenty of non-coding tasks such as helping design the software in Figma and contribute ideas. After the initial design stage, we will start coding.</p> <ul> <li>an eye for good design (find web apps/software you like and think about <em>why</em> you like it)</li> <li>empathy for the <strong>user</strong> of the software (likely you in the future!)</li> <li>Javascript experience</li> <li>React experience</li> <li>basic CSS skills</li> <li>motivation, and ability to work collaboratively</li> <li>basic Git skills (clone, branches, push, pull, rebase, merge) If you are lacking some of these skills that is perfectly fine. We can find contribution opportunities for you still!</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://neuosc.com/wp-content/uploads/2022/03/Flux-Logo-1-370x153.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/Neuronal-Oscillations/FLUX">FLUX: A pipeline for MEG analysis and beyond</a></h4> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://neuosc.com/flux/">FLUX </a> is a pipeline for analysing magnetoencephalography (MEG) data. By making the analyses steps and setting explicit, it aims to facilitate open science with the larger goal of improving the replicability of MEG research. So far, the FLUX pipeline has been developed for MNE-Python and FieldTrip with a focus on the MEGIN/Elekta system. The goal of this Brainhack project is to make the FLUX pipeline more flexible by making it fully BIDS compatible, as well as expanding its application to other systems, for instance CTF, optically pumped magnetometer (OPM) and electroencephalography (EEG).</p> </p> <h5>SKILLS</h5> <p><p>This is an ongoing project and there are many ways in which you could contribute; from helping to improve the documentation to developing new functionalities, all kinds of contributions are welcome. Any of the following skills will be very helpful:</p> <ul> <li>experience with MEG or any other neurophysiological method</li> <li>basic Python/MATLAB knowledge</li> <li>familiarity with BIDS</li> <li>good writing skills</li> <li>being enthusiastic about Neuroscience!</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://nipy.org/nibabel/_static/reggie.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nipy/nibabel/">Type hints for NiBabel</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Python has support for <a href="https://docs.python.org/3/library/typing.html">type annotations</a> to help developers code more effectively by catching bugs via static analysis or making auto-complete suggestions. The more libraries that annotate their code with useful type hints, the more effective this assistance becomes. The goal of this project is to annotate NiBabel to ease the development process for neuroimaging in Python and improve the reliability of code built on top of NiBabel. We will use <a href="http://mypy-lang.org/">mypy</a> for static analysis and test out type hinting in VScode.</p> </p> <h5>SKILLS</h5> <p><p>Minimum</p> <ul> <li>Some experience with Python and numpy</li> </ul> <p>Ideal</p> <ul> <li>Familiarity with some NiBabel APIs</li> <li>Experience with the <a href="https://docs.python.org/3/library/typing.html">typing</a> and <a href="https://numpy.org/devdocs/reference/typing.html">numpy.typing</a></li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/jsheunis/ohbm-2022/main/pics/datacat0_hero.svg" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/datalad/datalad-catalog">DataCat: "bring your own data" and auto-generate user-friendly data catalogs</a></h4> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><h4 id="summary">Summary</h4> <p>Do you want to learn how to generate a pretty and F.A.I.R. browser-based data catalog from metadata? Do you want to know how you can make your data known to the world, without sharing the actual data content on centralised infrastructure? Do you want to do this for free using open-source tools? YES?! Then “bring” your own data and join our hackathon project!</p> <h4 id="overview">Overview</h4> <p>DataLad Catalog is a free and open source command line tool, with a Python API, that assists with the automatic generation of user-friendly, browser-based data catalogs from structured metadata. It is an extension to <a href="https://www.datalad.org/">DataLad</a>, and together with <a href="https://github.com/datalad/datalad-metalad">DataLad Metalad</a> it brings distributed metadata handling, catalog generation, and maintenance into the hands of users. For a live example of a catalog that was generated using DataLad Catalog, see our <a href="https://datalad.github.io/datalad-catalog/">StudyForrest Demo</a>. The tool is now ready to be tested (and hopefully broken and then fixed!) on a wider range of user data. This is therefore intended to be a “bring your own data” project. If you are interested in metadata handling of (distributed) datasets, and specifically in generating a live catalog from said metadata, join us for a chance to turn your (metadata)data into a pretty browser application!</p> <h4 id="project-goals">Project Goals</h4> <ul> <li>Getting participants up to speed on what DataLad Catalog is and what it can do. This will be done through an initial discussion and by reading the <a href="https://github.com/datalad/tutorials/blob/master/notebooks/catalog_tutorials/datalad_catalog_primer.ipynb">primer</a></li> <li>Giving participants hand-on experience with the catalog generation process, with the use of <a href="https://github.com/datalad/tutorials/blob/master/notebooks/catalog_tutorials">walk-through tutorials</a></li> <li>Creating your own data catalogs</li> <li>Documenting feedback on your experience by creating issues (any and all types of issues are welcome!)</li> <li>Onboarding anyone interested in contributing to this tool in the many ways that are possible</li> </ul> </p> <h5>SKILLS</h5> <p><p>We welcome all kinds of contributions from various skills at any level. From setting up and writing documentation, discussing relevant functionality, or user-experience-testing, to Python-based implementation of the desired functionality and creating real-world use cases and workflows. You can help us with any of the following skills:</p> <ul> <li>You have a dataset (or distributed datasets) for which you’d like to create an online catalog</li> <li>You enjoy breaking user interfaces or pointing out how the interface can be more intuitive</li> <li>You have experience with the Unix command line</li> <li>You are interested in creating accessible documentation</li> <li>You know Python / JavaScript / HTML / VueJS</li> <li>You are interested in learning about the DataLad ecosystem or the process of creating a DataLad extension</li> <li>You are interested in learning about the DataLad metadata handling capabilities and/or the process of creating a DataLad-based metadata extractors</li> <li>You have knowledge of metadata standards in your domain</li> <li>You have knowledge of <a href="https://bids-specification.readthedocs.io/">BIDS</a> and <a href="https://github.com/bids-standard/pybids">pybids</a> (for the specific case of generating BIDS-related metadata, and rendering that in the catalog)</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neuronets/">Nobrainer toolkit and model zoo for deep learning in neuroimaging</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>The goals of this hackathon are to improve usability of both the nobrainer library and the nobrainer model zoo.</p> </p> <h5>SKILLS</h5> <p><ul> <li>Python</li> </ul> <p>Optional:</p> <ul> <li>Tensorflow</li> <li>Docker/Singularity</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://uvaauas.figshare.com/ndownloader/files/27251792/preview/27251792/preview.jpg" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/piloubazin/AHEAD-brains">Exploring the AHEAD brains together</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>We recently made available a post-mortem data set including quantitative MRI and microscopy reconstructed in 3D at 200µm (see <a href="https://doi.org/10.1126/sciadv.abj7892">this article</a> for details). The data set is openly accessible on FigShare already, but we would like to do more to help integrate it in other open science platforms to promote collaborative exploration of the data.</p> <p>Goals for the Brainhack:</p> <ul> <li>set up a version of the data set that fits into <a href="https://brainbox.pasteur.fr/">Brainbox</a></li> <li>check how the data is handled by various visualization tools, make recommendations</li> <li>import initial parcellations from automated tools (nighres so far, but others could be run) into the visualizations</li> <li>manually annotate errors, artifacts, inconsistencies</li> <li>delineate new structures collaboratively</li> </ul> </p> <h5>SKILLS</h5> <p><ul> <li>enthusiasm for detail neuroanatomy</li> <li>dealing with data (re)formatting, header manipulation</li> <li>experience with various brain visualization tools</li> <li>ideas to increase collaboration in neuroanatomical atlasing</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://raw.githubusercontent.com/nilearn/nilearn/main/doc/logos/nilearn-logo.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/nilearn/nilearn">Nilearn: Statistics for Neuroimaging in Python</a></h4> <h6> Hubs: Europe / Middle East / Africa, Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Nilearn is an open-source Python package for fast and easy analysis and visualization of brain images. It provides statistical and machine-learning tools, with instructive documentation and a friendly community. It includes applications such as multi-voxel pattern analysis (MVPA), decoding, predictive modelling, functional connectivity, and brain parcellations. Moreover, in recent years, Nilearn has expanded to include Generalized Linear Models (GLMs) to analyse functional MRI data. For the Brainhack, we aim to get feedback from the community about some of the highlights of the latest release. These include:</p> <ul> <li>A new module <a href="https://nilearn.github.io/stable/modules/reference.html#module-nilearn.interfaces">nilearn.interfaces</a> to implement loading and saving utilities with various interfaces</li> <li>Ability to provide <a href="https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_hrf.html#sphx-glr-auto-examples-04-glm-first-level-plot-hrf-py">custom hemodynamic response function (HRF)</a> for dealing with non human primate data in GLM analysis</li> <li><a href="https://nilearn.github.io/stable/auto_examples/01_plotting/plot_3d_map_to_surface_projection.html#interactive-plotting-with-plotly">Interactive surface plotting</a> using Plotly engine</li> <li>Improved <a href="https://nilearn.github.io/stable/development.html">contributing documentation</a></li> </ul> <p>Finally, we are always looking to get feedback on our <a href="https://nilearn.github.io/stable/index.html">documentation</a> and to onboard new contributors.</p> </p> <h5>SKILLS</h5> <p><p>We welcome all contributions from various skill sets and levels. This can include opening discussions around improvements to the <a href="https://nilearn.github.io/stable/index.html">documenation</a> and/or <a href="https://github.com/nilearn/nilearn">code base</a>, answering or commenting on questions or <a href="https://github.com/nilearn/nilearn/issues">issues raised on github</a> and <a href="https://neurostars.org/tag/nilearn">neurostars</a>, reviewing <a href="https://github.com/nilearn/nilearn/pulls">pull requests</a>, and <a href="https://nilearn.github.io/stable/development.html#how-to-contribute-to-nilearn">contributing code</a>.</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://user-images.githubusercontent.com/5311102/166160831-a81f55c3-c131-4e12-ab1f-e3f46593c9e5.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/danielemarinazzo/HOI">Higher order informational interactions in neuroimaging</a></h4> <h6> Hubs: Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://www.nature.com/articles/s41593-022-01070-0">Higher order interactions are being increasingly used and applied to neuroimaging data</a> To date there isn’t a freely available python toolbox with proper input/output suitable for neuroimaging data. We have a pretty much optimized <a href="https://github.com/danielemarinazzo/HOI">matlab code</a>, and <a href="https://github.com/PranavMahajan25/HOI_toolbox">a functioning, yet not optimized python one</a> by @PranavMahajan25 (with <a href="https://github.com/brainets/hoi_bhk/tree/main/etienne">some improvements</a> by @EtienneCmb). The goal(s) would be:</p> <ul> <li>improve the python implementation, adding statistical tests which are absent at the moment, adapting them from the matlab repository</li> <li>improve speed</li> <li>add input/output from BIDS processed data (MNE/NiLearn)</li> <li>explore solutions for plotting the data (<a href="https://github.com/renzocom/hyperplot">some attempts</a> by @renzocom) <a href="https://discord.gg/VUrQGnF8bc">Discord server for the project</a></li> </ul> </p> <h5>SKILLS</h5> <p><p>Python Matlab would help for the translation, but we can assist Some notions of statistics/probability theory could also help, but not necessary</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://ohbm-environment.org/wp-content/uploads/2021/12/logo_long-1.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurodatascience/watts_up_compute">watts_up_compute</a></h4> <h6> Hubs: Glasgow </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Integration of compute-tracker tools into neuroimaging pipelines to estimate carbon footprint of image processing. This is an ongoing project by the Sustainability and Environmental Action group (<a href="https://ohbm-environment.org/">SEA-SIG</a>) at the Organisation for Human Brain Mapping (OHBM). In this project we aim at better understanding the environmental costs of commonly used research pipelines and develop tools to help reduce them. Recently there have been several projects that track cpu/gpu “power draws” incurred during a compute task. These statistics can then be translated into carbon-footprint based on your location and time of processing.</p> <p>Available trackers</p> <ol> <li><a href="https://github.com/mlco2/codecarbon">CodeCarbon</a></li> <li><a href="https://github.com/lfwa/carbontracker">CarbonTracker</a></li> <li><a href="https://github.com/Breakend/experiment-impact-tracker">EIT</a></li> </ol> <p>Available trackers</p> <ol> <li>General purpose <a href="https://github.com/neurodatascience/watts_up_compute">wrapper</a> with CodeCarbon and EIT</li> <li>fMRIPrep <a href="https://github.com/nikhil153/fmriprep/blob/carbon-trackers/singularity/carbon_trackers_readme.md">integration</a> with CodeCarbon</li> </ol> <p>Available trackers</p> <ol> <li>Test fMRIPrep integration on multiple hardware</li> <li>Integrate trackers into other neuroimaging pipelines e.g. FSL, SPM etc.</li> </ol> </p> <h5>SKILLS</h5> <p><p>You don’t need to be familiar with all of these, just any subset of these would do!</p> <p>Programming languages</p> <ul> <li>Python</li> <li>Bash</li> <li>Matlab</li> </ul> <p>Neuro-software specific skills</p> <ul> <li>FreeSurfer</li> <li>fMRIPrep</li> <li>NiPype</li> <li>SPM</li> <li>FSL</li> </ul> <p>Data standards</p> <ul> <li>Brain Imaging Data Structure (BIDS)</li> </ul> <p>Git skills</p> <ul> <li>Git - 2: comfortable working with branches and can do a pull request on another repository</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://user-images.githubusercontent.com/29738718/170998973-86081990-33b6-45c2-9d77-21e68aea9053.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/datalad/datalad-dataverse">DataLad-Dataverse integration</a></h4> <h6> Hubs: Glasgow, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p><a href="https://dataverse.org">Dataverse</a> is open source research data repository software that is deployed all over the world in data or metadata repositories, so called Dataverse collections. It supports sharing, preserving, citing, exploring, and analyzing research data with descriptive metadata, and thus contributes greatly to open, reproducible, and FAIR science. <a href="https://www.datalad.org">DataLad</a>, on the other hand, is a data management and data publication tool build on <a href="https://git-scm.org">Git</a> and <a href="https://git-annex.branchable.com">git-annex</a>. Its core data structure, DataLad datasets, can version control files of any size, and streamline data sharing, updating, and collaboration. In this hackathon project, we aim to make DataLad interoperable with Dataverse to support dataset transport from and to Dataverse instances. To this end, we will build a new DataLad extension <code class="language-plaintext highlighter-rouge">datalad-dataverse</code>, and would be delighted to welcome <strong>you</strong> onboard of the contributor team.</p> </p> <h5>SKILLS</h5> <p><p>We plan to start from zero with this project, and welcome all kinds of contributions from various skills at any level. From setting up and writing documentation, discussing relevant functionality, or user-experience-testing, to Python-based implementation of the desired functionality and creating real-world use cases and workflows. You can help us with any of the following skills: You have used a Dataverse instance before and/or have access to one, or you are interested in using one in the future - You know technical details about Dataverse, such as its API, or would have fun finding out about them - You know Python - You have experience with the Unix command line - You are interested in creating accessible documentation - You are interested in learning about the DataLad ecosystem or the process of creating a DataLad extension - Your secret hobby is Git plumbing - You know git-annex, and/or about its backends - You want to help create metadata extractors for Dataverse to generate dataset metadata automatically</p> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://layerfmri.files.wordpress.com/2022/05/image-01.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://layerfmri.com/hackathon22/">MOSAIC for VASO fMRI</a></h4> <h6> Hubs: Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>Vascular Space Occupancy is an fMRI method that is popular for high-resolution layer-fMRI. Currently, the most popular sequence is the one by Rüdiger Stirnberg from the DZNE in Bonn, which is actively being employed at more than 30 sites. This sequence concomitantly acquires fMRI BOLD and blood volume signals. In the SIEMENS reconstruction pipeline, these signals are mixed together within the same time series, which challenges its user friendliness. Specifically:</p> <ul> <li>The “raw” dicom2nii-converted time-series are not BIDS compatible (see <a href="https://github.com/bids-standard/bids-specification/issues/1001">https://github.com/bids-standard/bids-specification/issues/1001</a>).</li> <li>The order of odd and even BOLD and VASO image TRs is dependent on the nii-converter.</li> <li>Workarounds with 3D distortion correction, results in interpolation artifacts.</li> <li>Workarounds without MOSAIC decorators result in impracticable large data sizes.</li> </ul> <p>The goal of this Hackathon is to extend the 3D-MOSAIC to solve these constraints. This functor is commonly used to sort images by echo-times, by RF-channels, by magnitude and phase in the SIEMENS reconstruction pipeline into sets of mosaics . However currently, this functor does not yet support the dimensionality of SETs. In this project we seek to include SETs into the capabilities of the functor.</p> <p>Acknowledgements:</p> <p>This project is based on previous tests by Rüdiger Stirnberg and Philipp Ehses to isolate the cause of the problem. This project will be based on the mosaic functor that was originally developed by Ben Poser and is currently being further extended by Philipp Ehses. The compatibility of the “raw” data with BIDS are supported by BIDS extensions spear-headed by Remi Gau and supported by Daniel Handwerker. The Hackathon logistics across various internet platforms are kindly guided by our Hackathon mentor Faruk Gulban.</p> </p> <h5>SKILLS</h5> <p><ul> <li>SIEMENS ICE programming in VE</li> <li>C++</li> </ul> </p> </div> </div> <div class="hackathon-row float-left col-md-12"> <div class="hackathon-img-wrapper animated hiding" data-animation="fadeInLeft" data-delay="0"> <img class="img-responsive" src="https://neurocausal.github.io/images/author/inverted-logo_huf11703388d6cc09ec62cec26261f7e3e_54804_148x148_fit_box_2.png" alt="Hackathon"> </div> <div class="hackathon-details animated hiding" data-animation="fadeInRight" data-delay="0"> <h4><a href="https://github.com/neurocausal">NeuroCAUSAL - Development of an Open Source Platform for the Storage, Sharing, Synthesis and Meta-Analysis of Clinical Data</a></h4> <h6> Hubs: Americas, Europe / Middle East / Africa </h6> <h5>PROJECT DESCRIPTION</h5> <p><p>We wish to work with clinicians, neuroimagers, and software developers to develop an open source platform for the storage, sharing, synthesis and meta-analysis of human clinical data to the service of the clinical and cognitive neuroscience community so that the future of neuropsychology can be transdiagnostic, open, and FAIR.</p> <p>Following the steps of what enable similar transition in functional neuroimaging, we are breaking down the over-ambitious goal in two stages:</p> <ol> <li>Create a sort of spin-off of <a href="https://neuroquery.org/">Neuroquery</a> that only covers lesion-related data hence allowing causal inferences</li> <li>A <a href="https://neurovault.org/">Neurovault</a> kind of tool facilitating sharing of clinical data, which shall benefit from a sort of “neuropsyhcological BIDS formatting guidelines” (OHBM poster #2066 seems to have read our minds)</li> </ol> </p> <h5>SKILLS</h5> <p><p>We are very heterogeneous in our own skills sets &amp; levels and welcome all sorts of contributions 😄 <br /> The <strong>ontological issues</strong> we are facing require familiarity with <a href="https://github.com/neurocausal/neurocausal/issues/4">neurology</a> and/or <a href="https://github.com/neurocausal/neurocausal/issues/5">cognitive science</a>. These are contentious matters in the field and a perfect solution is not realistic: we seek the good compromise that will make this platform a useful tool for the broad community interested in the future of neuropsychology.<br /> The <strong>technical issues</strong> will benefit from people familiar with tools to scrap data from texts, train/test predictive models, generally speaking converting to code our pipeline of papers selection &gt; model fitting &gt; function-to-structure mapping visualization (<a href="https://github.com/neurocausal/neurocausal_data/issues/1">example</a>).<br /> We are aware we just started scratching the surface and will need lots of help on all fronts 🙏</p> </p> </div> </div> </div> </div> </section> <!-- End About Hackathon Section --> <!-- Begin Footer --> <footer id="footer" class="footer"> <div class="row"> <div class="pull-left col-md-6 col-xs-6"> <div class="g-plusone" data-size="medium" data-annotation="inline" data-width="300" data-href="https://ohbm.github.io/hackathon2022"></div> </div> <div class="logo logo-footer logo-gray pull-right"></div> </div> <div class="row"> <div class="col-md-4 col-xs-6"> <h5>Links</h5> <ul> <li><a href=" https://ossig.netlify.com/ " target="_blank">OHBM Open Science SIG</a></li> <li><a href=" https://www.brainhack.org/ " target="_blank">Brainhack Global</a></li> </ul> </div> <div class="col-md-4 col-xs-6"> <h5>Organizers and contacts</h5> <ul> <li><a href=" https://twitter.com/HaoTingW713 " target="_blank">Hao-Ting Wang (Brainhack Co-chair)</a></li> <li><a href=" https://twitter.com/SteMoia " target="_blank">Stefano Moia (Brainhack Co-chair)</a></li> <li><a href=" mailto:OHBMopenscience@gmail.com " target="_blank">OHBMopenscience@gmail.com</a></li> </ul> </div> <div class="col-md-4 col-xs-6"> <h5>Resources</h5> <ul> <li><a href=" /hackathon2022/coc " target="_blank">Code of Conduct</a></li> <li><a href=" /hackathon2022/contact/ " target="_blank">Contact</a></li> </ul> </div> </div> <div class="row"> <div class="col-md-6 col-xs-12"> <ul class="social-links"> <li> <a href=" https://twitter.com/ohbmopen " target="_blank"> <svg class="icon icon-twitter" viewBox="0 0 30 32"> <use xlink:href="/hackathon2022/img/sprites/sprites.svg#icon-twitter"></use> </svg> </a> </li> <li> <a href=" /hackathon2022/feed.xml " target="_blank"> <svg class="icon icon-rss" viewBox="0 0 30 32"> <use xlink:href="/hackathon2022/img/sprites/sprites.svg#icon-rss"></use> </svg> </a> </li> </ul> </div> </div> <div class="row"> <!-- Please don't delete this line--> <div class="col-md-6"> <p class="copyright"> &copy; 2018 Based on <a href="https://github.com/gdg-x/zeppelin" target="_blank">Project Zeppelin</a>. Designed and created by <a href="https://plus.google.com/+OlehZasadnyy/about" target="_blank">Oleh Zasadnyy</a> &middot; <br>Edited and re-adapted by Elizabeth Levitis and Remi Gau.</a> </p> </div> </div> </footer> <!-- End Footer --> </div> </div> </div> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', '', ''); ga('send', 'pageview'); </script> <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <script> window.jQuery || document.write('<script src="/hackathon2022/js/jquery-2.1.1.min.js><\/script>') </script> <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script> <script> if (typeof($.fn.modal) === 'undefined') { document.write('<script src="/hackathon2022/js/bootstrap.min.js><\/script>') } </script> <script src="/hackathon2022/js/default.js"></script> <script> Waves.displayEffect(); </script> <script src="/hackathon2022/js/scripts.js"></script> <script type="application/ld+json"> [{ "@context" : "http://schema.org", "@type" : "Event", "name" : "OHBM BrainHack 2022", "description": "OHBM Brainhack June 16-18th, 2022. Organized by the OHBM Open Science SIG.", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022", "url" : "https://ohbm.github.io/hackathon2022", "startDate" : "", "doorTime" : "", "endDate" : "", "location" : { "@type" : "Place", "name" : "", "sameAs" : "", "address" : { "@type" : "PostalAddress", "streetAddress" : "", "addressLocality" : "", "addressRegion" : "", "postalCode" : "", "addressCountry" : "" }, "geo" : { "@type" : "GeoCoordinates", "latitude" : "", "longitude" : "" } }, // Not supported yet // "organizer" : { // "@type" : "Organization", // "name" : "OHBM Open Science SIG", // "alternateName" : "", // "description" : "", // "logo" : "https://ohbm.github.io/hackathon2022/hackathon2022", // "email" : "", // "sameAs" : "" // }, "subEvent" : { "@type" : "Event", "name" : "HackTrack", "description": "", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022", "url" : "https://ohbm.github.io/hackathon2022/hackathon2022/hackathon/", "startDate" : "2022-06-16", "doorTime" : "09:00", "endDate" : "2022-06-16", "location" : { "@type" : "Place", "name" : "Queen Margaret Union", "sameAs" : "http://communa.net.ua/", "address" : { "@type" : "PostalAddress", "streetAddress" : "22 University Gardens", "addressLocality" : "Glasgow", "addressRegion" : "", "postalCode" : "G12 8QN", "addressCountry" : "United Kingdom" }, "geo" : { "@type" : "GeoCoordinates", "latitude" : "55.8735962", "longitude" : " -4.2913955" } } }, "offers" : [ { "@type" : "Offer", "name" : "Early Bird", "url" : "http://dfua.ticketforevent.com/", "price" : "350", "priceCurrency" : "UAH", "validFrom" : "2014-08-25T10:00", "validThrough" : "2014-09-30T23:59" } ], "performer" : [ { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" }, { "@type" : "Person", "name" : " ", "image" : "https://ohbm.github.io/hackathon2022/hackathon2022/img/people/", "jobTitle" : "", "worksFor" : { "@type" : "Organization", "name" : "" }, "sameAs" : "" } ], "eventStatus" : "EventScheduled", "typicalAgeRange" : "16+" }] </script> </body> </html>
